{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>ADX-Mon is a comprehensive observability platform that seamlessly integrates metrics, logs, traces, continuous profile and any telemetry into a unified platform. It addresses the traditional challenges of data being siloed and difficult to correlate, as well as the cardinality and scale issues found in existing metrics solutions, streamlining the collection and analysis of observability data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unlimited Cardinality, Retention, and Granularity: No restrictions on data dimensions, storage duration, or detail level.</li> <li>Low Latency Ingestion and Lightweight Collection: Ensures rapid data processing with minimal resource overhead.</li> <li>Unified Alerting: Simplified alerting mechanisms for metrics, logs, and traces.</li> <li>Powered by Azure Data Explorer (ADX): Built on the robust and scalable ADX platform with the powerful Kusto Query Language (KQL) unifying access to all telemetry.</li> <li>Flexible and Standards-Based Interfaces: Supports OTEL, Prometheus, and other industry standards ingestion protocols.</li> <li>Optimized for Kubernetes and Cloud-Native Environments: Designed to thrive in modern, dynamic infrastructures.</li> <li>Pluggable Alerting Provider API: Customizable alerting integrations.</li> <li>Broad Compatibility: Works seamlessly with Grafana, ADX Dashboards, PowerBI, and any ADX-compatible products.</li> <li>Turnkey, Scalable, and Reliable: A ready-to-use observability platform that scales from hobby projects to enterprise-level deployments.</li> </ul>"},{"location":"#learn-more","title":"Learn More","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Concepts</li> <li>Cook Book </li> </ul>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#overview","title":"Overview","text":"<p>ADX-Mon is a fully managed observability solution that supports metrics, logs and traces in a unified stack. The entrypoint to ADX-Mon is the <code>collector</code> which is deployed as a daemonset in your Kubernetes cluster.</p> <p>The collector is responsible for collecting metrics, logs and traces from your Kubernetes cluster and sending them to the <code>ingestor</code> endpoint which handles the ingestion of data into Azure Data Explorer (ADX).</p> <p>All collected data is translated to ADX tables.  Each table has a consistent schema that can be extended through <code>update policies</code> to pull commonly used labels and attributes up to top level columns.</p> <p>These tables are all queried with KQL.  KQL queries are used for analysis, alerting and visualization.</p>"},{"location":"concepts/#components","title":"Components","text":""},{"location":"concepts/#collector","title":"Collector","text":""},{"location":"concepts/#ingestor","title":"Ingestor","text":""},{"location":"concepts/#alerter","title":"Alerter","text":""},{"location":"concepts/#azure-data-explorer","title":"Azure Data Explorer","text":""},{"location":"concepts/#grafana","title":"Grafana","text":""},{"location":"concepts/#telemetry","title":"Telemetry","text":""},{"location":"concepts/#metrics","title":"Metrics","text":"<p>Metrics track a numeric value over time with associated labels to identify series.  Metrics are collected from Kubernetes via the Prometheus scrape protocol as well as received via prometheus remote write protocol and OTLP metrics protocol.</p> <p>Metrics are translated to a distinct table per metric.  Each metric table has the following columns:</p> <ul> <li><code>Timestamp</code> - The timestamp of the metric.</li> <li><code>Value</code> - The value of the metric.</li> <li><code>Labels</code> - A dynamic column that contains all labels associated with the metric.</li> <li><code>SeriesId</code> - A unique ID for the metric series that comprises the <code>Labels</code> and metric name.</li> </ul> <p>Labels may have common identifying attributes that can be pulled up to top level columns via update policies.  For example, the <code>pod</code> label may be common to all metrics and can be pulled up to a top level <code>Pod</code> column.</p>"},{"location":"concepts/#logs","title":"Logs","text":"<p>Logs are ingested into ADX as OTLP records. You can define custom table schemas through a Kubernetes CRD called <code>Function</code>, which represents an ADX View. This allows you to present log events in a custom format rather than querying the OTLP structure directly. Below is an example of specifying a custom schema for the Ingestor component:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Function\nmetadata:\n  name: ingestor-view\n  namespace: default\nspec:\n  body: |\n    .create-or-alter function with (view=true, folder='views') Ingestor () {\n      table('Ingestor')\n      | project msg = tostring(Body.msg),\n          lvl = tostring(Body.lvl),\n          ts = todatetime(Body.ts),\n          namespace = tostring(Body.namespace),\n          container = tostring(Body.container),\n          pod = tostring(Body.pod),\n          host = tostring(Body.host) \n    }\n  database: Logs\n</code></pre> <p>Naming the View the same as the Table ensures the View takes precedence when queried in ADX. For example:</p> <pre><code>Ingestor\n| where ts &gt; ago(1h)\n| where lvl == 'ERR'\n</code></pre>"},{"location":"concepts/#traces","title":"Traces","text":""},{"location":"concepts/#continuous-profiling","title":"Continuous Profiling","text":""},{"location":"concepts/#alerts","title":"Alerts","text":"<p>Alerts are defined through a Kubernetes CRD called <code>AlertRule</code>.  This CRD defines the alerting criteria and the notification channels that should be used when the alert is triggered.</p> <p>Alerts are triggered when the alerting criteria is met.  The alerting criteria is defined as a KQL query that is executed against the ADX cluster.  The query is executed on a schedule and if the query returns any results, the alert triggers.  Each row of the result translates into an alert notification.</p> <p>Below is a sample alert on a metric.</p> <pre><code>---\napiVersion: adx-mon.azure.com/v1\nkind: AlertRule\nmetadata:\n  name: unique-alert-name\n  namespace: alert-namespace\nspec:\n  database: SomeDatabase\n  interval: 5m\n  query: |\n    let _from=_startTime-1h;\n    let _to=_endTime;\n    KubePodContainerStatusWaitingReason\n    | where Timestamp between (_from .. _to)\n    | where ...\n    | extend Container=tostring(Labels.container), Namespace=tostring(Labels.namespace), Pod=tostring(Labels.pod)\n    | extend Severity=3\n    | extend Title=\"Alert tittle\"\n    | extend Summary=\"Alert summary details\"\n    | extend CorrelationId=\"Unique ID to correlate alerts\"\n  autoMitigateAfter: 1h\n  destination: \"alerting provider destination\"\n  criteria:\n    cloud:\n      - AzureCloud\n</code></pre> <p>All must have the following fields:</p> <ul> <li><code>database</code> - The ADX database to execute the query.</li> <li><code>interval</code> - The interval at which the query should be executed.</li> <li><code>query</code> - The KQL query to execute.</li> <li><code>destination</code> - The destination to send the alert to.  This is provider specific.</li> </ul> <p>The query must return a table with the following columns:</p> <ul> <li><code>Severity</code> - The severity of the alert.  This is used to determine the priority of the alert.</li> <li><code>Title</code> - The title of the alert.</li> <li><code>Summary</code> - The summary of the alert.</li> <li><code>CorrelationId</code> - A unique ID to correlate alerts.  A correlation ID is necessary to prevent duplicate alerts from being sent to the destination.  If one is not specified, a new alert will be created each interval.</li> </ul> <p>Optionally, the query can return the following fields:</p> <ul> <li><code>autoMitigateAfter</code> - The amount of time after the alert is triggered that it should be automatically mitigated if it has not correlated.  If a <code>CorrelationId</code> is specified, this field is ignored.</li> <li><code>criteria</code> - A list of criteria that must be met for the alert to trigger.  If not specified, the alert will trigger in all environments.  This is useful for alerts that should only trigger in a specific cloud or region.  The available criteria options are determined by the <code>alerter</code> tag settings.</li> </ul>"},{"location":"config/","title":"Config","text":""},{"location":"config/#collector-config","title":"Collector Config","text":"<p>Collector is configured with a TOML-formatted file. In Kubernetes deployments, this is typically within a ConfigMap mounted into the collector pod. A default config can be generated by running <code>./collector config</code>.</p>"},{"location":"config/#global-config","title":"Global Config","text":"<p>This is the top level configuration for the collector. The only required fields are <code>Endpoint</code> and <code>StorageDir</code>.</p> <pre><code># Ingestor URL to send collected telemetry.\nendpoint = 'https://ingestor.adx-mon.svc.cluster.local'\n# Path to kubernetes client config\nkube-config = '.kube/config'\n# Skip TLS verification.\ninsecure-skip-verify = true\n# Address to listen on for endpoints.\nlisten-addr = ':8080'\n# Region is a location identifier.\nregion = 'eastus'\n# Optional path to the TLS key file.\ntls-key-file = '/etc/certs/collector.key'\n# Optional path to the TLS cert bundle file.\ntls-cert-file = '/etc/certs/collector.pem'\n# Maximum number of connections to accept.\nmax-connections = 100\n# Maximum number of samples to send in a single batch.\nmax-batch-size = 1000\n# Max segment agent in seconds.\nmax-segment-age-seconds = 30\n# Maximum segment size in bytes.\nmax-segment-size = 52428800\n# Maximum allowed size in bytes of all segments on disk.\nmax-disk-usage = 53687091200\n# Interval to flush the WAL. (default 100)\nwal-flush-interval-ms = 100\n# Storage directory for the WAL and log cursors.\nstorage-dir = '/var/lib/adx-mon'\n# Enable pprof endpoints.\nenable-pprof = true\n# Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\ndefault-drop-metrics = false\n# Global Regexes of metrics to drop.\ndrop-metrics = [\n  '^kube_pod_ips$',\n  'etcd_grpc.*'\n]\n# Global Regexes of metrics to keep.\nkeep-metrics = [\n  'nginx.*'\n]\n# Attributes lifted from the Body field and added to Attributes.\nlift-attributes = [\n  'host'\n]\n\n# Global Key/value pairs of labels to add to all metrics.\n[add-labels]\n  collectedBy = 'collector'\n\n# Global labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;. These are dropped from all metrics collected by this agent\n[drop-labels]\n  '^nginx_connections_accepted' = '^pid$'\n\n# Global Regexes of metrics to keep if they have the given label and value. These are kept from all metrics collected by this agent\n[[keep-metrics-with-label-value]]\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  label-regex = 'owner'\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  value-regex = 'platform'\n\n[[keep-metrics-with-label-value]]\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  label-regex = 'type'\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  value-regex = 'frontend|backend'\n\n# Global labels to lift from the metric to top level columns\n[[lift-labels]]\n  # The name of the label to lift.\n  name = 'Host'\n  # The name of the column to lift the label to.\n  column = ''\n\n[[lift-labels]]\n  # The name of the label to lift.\n  name = 'cluster_name'\n  # The name of the column to lift the label to.\n  column = 'Cluster'\n\n# Key/value pairs of attributes to add to all logs.\n[add-attributes]\n  cluster = 'cluster1'\n  geo = 'eu'\n\n# Optional configuration for exporting telemetry outside of adx-mon in parallel with sending to ADX.\n# Exporters are declared here and referenced by name in each collection source.\n[exporters]\n  # Configuration for exporting metrics to an OTLP/HTTP endpoint.\n  [[exporters.otlp-metric-export]]\n    # Name of the exporter.\n    name = 'to-otlp'\n    # OTLP/HTTP endpoint to send metrics to.\n    destination = 'http://localhost:4318/v1/metrics'\n    # Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\n    default-drop-metrics = true\n    # Regexes of metrics to drop.\n    drop-metrics = []\n    # Regexes of metrics to keep.\n    keep-metrics = [\n      '^kube_pod_ips$'\n    ]\n    # Regexes of metrics to keep if they have the given label and value.\n    keep-metrics-with-label-value = []\n\n    # Key/value pairs of labels to add to all metrics.\n    [exporters.otlp-metric-export.add-labels]\n      forwarded_to = 'otlp'\n\n    # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n    [exporters.otlp-metric-export.drop-labels]\n      '^kube_pod_ips$' = '^ip_family'\n\n    # Key/value pairs of resource attributes to add to all metrics.\n    [exporters.otlp-metric-export.add-resource-attributes]\n      destination_namespace = 'prod-metrics'\n</code></pre>"},{"location":"config/#prometheus-scrape","title":"Prometheus Scrape","text":"<p>Prometheus scrape discovers pods with the <code>adx-mon/scrape</code> annotation as well as any defined static scrape targets. It ships any metrics to the defined ADX database.</p> <pre><code># Defines a prometheus format endpoint scraper.\n[prometheus-scrape]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # Scrape interval in seconds.\n  scrape-interval = 10\n  # Scrape timeout in seconds.\n  scrape-timeout = 5\n  # Disable metrics forwarding to endpoints.\n  disable-metrics-forwarding = false\n  # Disable discovery of kubernetes pod targets.\n  disable-discovery = false\n  # Regexes of metrics to drop.\n  drop-metrics = [\n    '^kube_pod_ips$',\n    'etcd_grpc.*'\n  ]\n  # Regexes of metrics to keep.\n  keep-metrics = [\n    'nginx.*'\n  ]\n  # List of exporter names to forward metrics to.\n  exporters = []\n\n  # Defines a static scrape target.\n  [[prometheus-scrape.static-scrape-target]]\n    # The regex to match the host name against.  If the hostname matches, the URL will be scraped.\n    host-regex = '.*'\n    # The URL to scrape.\n    url = 'http://localhost:9090/metrics'\n    # The namespace label to add for metrics scraped at this URL.\n    namespace = 'monitoring'\n    # The pod label to add for metrics scraped at this URL.\n    pod = 'host-monitor'\n    # The container label to add for metrics scraped at this URL.\n    container = 'host-monitor'\n\n  # Regexes of metrics to keep if they have the given label and value.\n  [[prometheus-scrape.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'owner'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'platform'\n\n  [[prometheus-scrape.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'type'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'frontend|backend'\n</code></pre>"},{"location":"config/#prometheus-remote-write","title":"Prometheus Remote Write","text":"<p>Prometheus remote write accepts metrics from Prometheus remote write protocol. It ships metrics to the defined ADX database.</p> <pre><code># Defines a prometheus remote write endpoint.\n[[prometheus-remote-write]]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # The path to listen on for prometheus remote write requests.  Defaults to /receive.\n  path = '/receive'\n  # Regexes of metrics to drop.\n  drop-metrics = [\n    '^kube_pod_ips$',\n    'etcd_grpc.*'\n  ]\n  # Regexes of metrics to keep.\n  keep-metrics = [\n    'nginx.*'\n  ]\n  # List of exporter names to forward metrics to.\n  exporters = []\n\n  # Key/value pairs of labels to add to all metrics.\n  [prometheus-remote-write.add-labels]\n    cluster = 'cluster1'\n\n  # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n  [prometheus-remote-write.drop-labels]\n    '^nginx_connections_accepted' = '^pid$'\n\n  # Regexes of metrics to keep if they have the given label and value.\n  [[prometheus-remote-write.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'owner'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'platform'\n\n  [[prometheus-remote-write.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'type'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'frontend|backend'\n</code></pre>"},{"location":"config/#otel-log","title":"Otel Log","text":"<p>The Otel log endpoint accepts OTLP/HTTP logs from an OpenTelemetry sender. By default, this listens under the path <code>/v1/logs</code>.</p> <pre><code># Defines an OpenTelemetry log endpoint. Accepts OTLP/HTTP.\n[otel-log]\n  # Attributes lifted from the Body and added to Attributes.\n  lift-attributes = [\n    'host'\n  ]\n\n  # Key/value pairs of attributes to add to all logs.\n  [otel-log.add-attributes]\n    cluster = 'cluster1'\n    geo = 'eu'\n</code></pre>"},{"location":"config/#otel-metrics","title":"Otel Metrics","text":"<p>The Otel metrics endpoint accepts OTLP/HTTP and/or OTLP/gRPC metrics from an OpenTelemetry sender.</p> <pre><code># Defines an OpenTelemetry metric endpoint. Accepts OTLP/HTTP and/or OTLP/gRPC.\n[[otel-metric]]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # The path to listen on for OTLP/HTTP requests.\n  path = '/v1/otlpmetrics'\n  # The port to listen on for OTLP/gRPC requests.\n  grpc-port = 4317\n  # Regexes of metrics to drop.\n  drop-metrics = [\n    '^kube_pod_ips$',\n    'etcd_grpc.*'\n  ]\n  # Regexes of metrics to keep.\n  keep-metrics = [\n    'nginx.*'\n  ]\n  # List of exporter names to forward metrics to.\n  exporters = []\n\n  # Key/value pairs of labels to add to all metrics.\n  [otel-metric.add-labels]\n    cluster = 'cluster1'\n\n  # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.  These are dropped from all metrics collected by this agent\n  [otel-metric.drop-labels]\n    '^nginx_connections_accepted' = '^pid$'\n\n  # Regexes of metrics to keep if they have the given label and value.\n  [[otel-metric.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'owner'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'platform'\n\n  [[otel-metric.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'type'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'frontend|backend'\n</code></pre>"},{"location":"config/#host-log","title":"Host Log","text":"<p>The host log config configures file and journald log collection. By default, Kubernetes pods with <code>adx-mon/log-destination</code> annotation will have their logs scraped and sent to the appropriate destinations.</p> <pre><code># Defines a host log scraper.\n[[host-log]]\n  # Disable discovery of Kubernetes pod targets. Only one HostLog configuration can use Kubernetes discovery.\n  disable-kube-discovery = false\n  # Defines a list of transforms to apply to log lines.\n  transforms = []\n\n  # Key/value pairs of attributes to add to all logs.\n  [host-log.add-attributes]\n    cluster = 'cluster1'\n    geo = 'eu'\n\n  # Defines a tail file target.\n  [[host-log.file-target]]\n    # The path to the file to tail.\n    file-path = '/var/log/nginx/access.log'\n    # The type of log being output. This defines how timestamps and log messages are extracted from structured log types like docker json files. Options are: docker, plain.\n    log-type = 'plain'\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'NginxAccess'\n    # Parsers to apply sequentially to the log line.\n    parsers = []\n\n  [[host-log.file-target]]\n    # The path to the file to tail.\n    file-path = '/var/log/myservice/service.log'\n    # The type of log being output. This defines how timestamps and log messages are extracted from structured log types like docker json files. Options are: docker, plain.\n    log-type = 'plain'\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'NginxAccess'\n    # Parsers to apply sequentially to the log line.\n    parsers = [\n      'json'\n    ]\n\n  # Defines a journal target to scrape.\n  [[host-log.journal-target]]\n    # Matches for the journal reader based on journalctl MATCHES. To select a systemd unit, use the field _SYSTEMD_UNIT. (e.g. '_SYSTEMD_UNIT=avahi-daemon.service' for selecting logs from the avahi-daemon service.)\n    matches = [\n      '_SYSTEMD_UNIT=docker.service',\n      '_TRANSPORT=journal'\n    ]\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'Docker'\n    # Parsers to apply sequentially to the log line.\n    parsers = []\n</code></pre>"},{"location":"config/#exporters","title":"Exporters","text":"<p>Exporters are used to send telemetry to external systems in parallel with data sent to Azure Data Explorer. The collector currently supports sending metrics to OpenTelemetry OTLP/HTTP endpoints. Exporters are defined under the top level configuration key <code>exporters</code>under the exporter type. They are referenced by name in each metric collector.</p> <p>Metric collectors process metrics through their own metric filters and transforms prior to forwarding them to any defined exporters. The exporters then apply their own filters and transforms before sending the metrics to the destination.</p> <pre><code># Defines a prometheus format endpoint scraper.\n[prometheus-scrape]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # Defines a static scrape target.\n  static-scrape-target = []\n  # Scrape interval in seconds.\n  scrape-interval = 10\n  # Scrape timeout in seconds.\n  scrape-timeout = 5\n  # Disable metrics forwarding to endpoints.\n  disable-metrics-forwarding = false\n  # Disable discovery of kubernetes pod targets.\n  disable-discovery = false\n  # Regexes of metrics to drop.\n  drop-metrics = []\n  # Regexes of metrics to keep.\n  keep-metrics = []\n  # Regexes of metrics to keep if they have the given label and value.\n  keep-metrics-with-label-value = []\n  # List of exporter names to forward metrics to.\n  exporters = [\n    'to-local-otlp',\n    'to-remote-otlp'\n  ]\n\n# Optional configuration for exporting telemetry outside of adx-mon in parallel with sending to ADX.\n# Exporters are declared here and referenced by name in each collection source.\n[exporters]\n  # Configuration for exporting metrics to an OTLP/HTTP endpoint.\n  [[exporters.otlp-metric-export]]\n    # Name of the exporter.\n    name = 'to-local-otlp'\n    # OTLP/HTTP endpoint to send metrics to.\n    destination = 'http://localhost:4318/v1/metrics'\n    # Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\n    default-drop-metrics = true\n    # Regexes of metrics to drop.\n    drop-metrics = []\n    # Regexes of metrics to keep.\n    keep-metrics = [\n      '^kube_pod_ips$'\n    ]\n    # Regexes of metrics to keep if they have the given label and value.\n    keep-metrics-with-label-value = []\n\n    # Key/value pairs of labels to add to all metrics.\n    [exporters.otlp-metric-export.add-labels]\n      forwarded_to = 'otlp'\n\n    # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n    [exporters.otlp-metric-export.drop-labels]\n      '^kube_pod_ips$' = '^ip_family'\n\n    # Key/value pairs of resource attributes to add to all metrics.\n    [exporters.otlp-metric-export.add-resource-attributes]\n      destination_namespace = 'prod-metrics'\n\n  [[exporters.otlp-metric-export]]\n    # Name of the exporter.\n    name = 'to-remote-otlp'\n    # OTLP/HTTP endpoint to send metrics to.\n    destination = 'https://metrics.contoso.org/v1/metrics'\n    # Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\n    default-drop-metrics = true\n    # Regexes of metrics to drop.\n    drop-metrics = []\n    # Regexes of metrics to keep.\n    keep-metrics = [\n      '^service_hit_count$',\n      '^service_latency$'\n    ]\n    # Regexes of metrics to keep if they have the given label and value.\n    keep-metrics-with-label-value = []\n\n    # Key/value pairs of labels to add to all metrics.\n    [exporters.otlp-metric-export.add-labels]\n      forwarded_to = 'otlp'\n\n    # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n    [exporters.otlp-metric-export.drop-labels]\n      '^service_hit_count$' = '^origin_ip$'\n\n    # Key/value pairs of resource attributes to add to all metrics.\n    [exporters.otlp-metric-export.add-resource-attributes]\n      destination_namespace = 'primary-metrics'\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#alerting","title":"Alerting","text":""},{"location":"cookbook/#detecting-rate-of-change-in-a-metric","title":"Detecting rate of change in a metric","text":""},{"location":"cookbook/#detecting-increase-in-error-messages","title":"Detecting increase in error messages","text":""},{"location":"cookbook/#metrics","title":"Metrics","text":""},{"location":"cookbook/#annotating-pods-for-collection","title":"Annotating pods for collection","text":""},{"location":"cookbook/#adding-static-scrape-targets","title":"Adding static scrape targets","text":""},{"location":"cookbook/#logging","title":"Logging","text":""},{"location":"cookbook/#instrumenting-logs-for-collection","title":"Instrumenting logs for collection","text":""},{"location":"guides/","title":"Guides","text":""},{"location":"ingestor/","title":"Ingestor Overview","text":"<p>The ingestor is an aggregation point for adx-mon to ingest metrics, logs and traces into Azure Data Explore (ADX) in a performant and cost-efficient manner.</p> <p>ADX recommends sending batches of data in 100MB to 1GB (uncompressed) [1] for optimal ingestion and reduced costs.  In a typical Kubernetes cluster, there are many pods, each with their own metrics, logs and traces.  The ingestor aggregates these data sources into batches and sends them to ADX instead of each pod or node sending data individually.  This reduces the number of small files that ADX must later  merge into larger files which can impact query latency and increase resource requirements.</p>"},{"location":"ingestor/#design","title":"Design","text":"<p>The ingestor is designed to be deployed as a Kubernetes StatefulSet with multiple replicas.  It exposes several ingress points for metrics, logs and traces collection.  The metrics ingress API is a Prometheus remote write endpoint and can support other interfaces such as OpenTelemetry in the future.  </p> <p>The ingestor can be dynamically scaled up or down based on the amount of data being ingested.  It has a configurable amount of storage to buffer data before sending it to ADX.  It will store and coalesce data until it reaches a maximum size or a maximum age.  Once either of these thresholds are reached, the data is sent to ADX.</p> <p>Several design decisions were made to optimize availability and performance.  For example, if a pod is able to recieve data it will store it locally and attempt to optimize it for upload to ADX later by transferring small segments to peers. The performance and throughput of a single ingestor pod is limited by network bandwidth and disk throughput of attached  storage.  The actual processing performed by the ingestor is fairly minimal and is mostly unmarshalling the incoming data (Protobufs) and writing it to disk in an append only storage format.</p>"},{"location":"ingestor/#data-flow","title":"Data Flow","text":""},{"location":"ingestor/#metrics","title":"Metrics","text":"<p>Each ingestor pod is fronted by a load balancer.  The ingestor receives data from the Kubernetes cluster via the  Prometheus remote write endpoint.  When a pod receives data, it writes it locally to a file that corresponds to a given table and schema.  These files are called Segments and are part of Write Ahead Log (WAL) for each table. </p> <p>If Segment has reached the max age or max size, the ingestor will either upload the file directly to ADX or transfer the file to a peer that is assigned to own that particular table.  The transfer is performed if the file is less than 100MB so that the file can be merged with other files before being uploaded to ADX.  </p> <p>If the transfer fails, the instance will upload the file directly. </p> <p>During upload, batches of files, per table, are compressed and uploaded to ADX as stream.  This allows many small files to be merged into a single file which reduces the number of files that ADX must merge later.  Each batch is sized to be between 100MB and 1GB (uncompressed) to align with Kusto ingestion best practices.</p>"},{"location":"ingestor/#logs","title":"Logs","text":""},{"location":"ingestor/#traces","title":"Traces","text":"<p>[1] https://docs.microsoft.com/en-us/azure/data-explorer/ingest-best-practices</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"quick-start/","title":"Quick Start","text":"<p>This guide will deploy ADX-Mon on an Azure Kubernetes Service (AKS) cluster and send collected telemetry to an Azure Data Explorer cluster.  It will deploy all components within the cluster and demonstrate  how to enable telemetry collection on a pod and query it from Azure Data Explorer.</p>"},{"location":"quick-start/#pre-requisites","title":"Pre-Requisites","text":"<p>You will need the following to complete this guide.</p> <ul> <li>An AKS cluster</li> <li>An Azure Data Explorer cluster</li> <li>A Linux environment with Azure CLI installed</li> </ul> <p>These clusters should be in the same region for this guid.  You should have full admin access to both clusters.</p>"},{"location":"quick-start/#deploy-adx-mon","title":"Deploy ADX-Mon","text":"<pre><code>bash &lt;(curl -s  https://raw.githubusercontent.com/Azure/adx-mon/main/build/k8s/bundle.sh)\n</code></pre> <p>This script will prompt you for the name or you AKS and ADX cluster and configure them to accept telemetry from ADX-Mon components. It configures the provided ADX cluster with <code>Metrics</code> and <code>Logs</code> databases and deploy the Collector and Ingestor services to begin collecting and shipping data from the AKS cluster.</p>"},{"location":"quick-start/#annotate-your-pods","title":"Annotate Your Pods","text":"<p>Telemetry can be ingested into ADX-Mon by annotating your pods with the appropriate annotations or shipping it through OTEL endpoints.  The simplest model is to annotate your pods with the appropriate annotations.</p>"},{"location":"quick-start/#metrics","title":"Metrics","text":"<p>ADX-Mon collector support scraping Prometheus style endpoints directly. To enable this, annotate your pods with these annotations, configuring the port and path to match the port and http path for the metrics endpoint of your service.</p> <p>Prometheus metric names will be transformed from <code>snake_case</code> to <code>TitleCase</code>. As an example, <code>adxmon_collector_logs_sent</code> is transformed into <code>AdxmonCollectorLogsSent</code> when sent to Kusto.</p> <pre><code>adx-mon/scrape: \"true\"\nadx-mon/port: \"8080\"\nadx-mon/path: \"/metrics\"\n</code></pre>"},{"location":"quick-start/#logs","title":"Logs","text":"<p>ADX-Mon collector supports discovering logs from pods. To configure the destination Kusto table, annotate your pod with <code>adx-mon/log-destination</code> with a value of <code>DBName:TableName</code>.</p> <p>By default, collector parses each log line as plaintext, but an optional list of log-parsers can be defined as a comma separated list. It currently supports json-formatted log lines in addition to plaintext.</p> <pre><code>adx-mon/scrape: \"true\"\nadx-mon/log-destination: \"Logs:Collector\"\nadx-mon/log-parsers: json\n</code></pre>"},{"location":"quick-start/#query-your-data","title":"Query Your Data","text":"<p>After bootstrapping, the provided ADX cluster will begin to populate with metrics and logs. The <code>Metrics</code> database is configured with a default <code>30s</code> batch latency for ADX ingestion to optimize for latency, while <code>Logs</code> is configured with a default of <code>5m</code> to optimize for throughput.</p>"},{"location":"quick-start/#metric-examples","title":"Metric Examples","text":"<pre><code>// Process a prometheus-style counter to determine the number of logs sent by a given source in Collector in the last hour\nAdxmonCollectorLogsSent\n| where Timestamp &gt; ago(1h)\n// convert from point-in-time count to amount of increase per interval\n| invoke prom_delta()\n| summarize TotalSent=sum(Value) by Host=tostring(Labels.host), Source=tostring(Labels.source)\n</code></pre>"},{"location":"quick-start/#log-examples","title":"Log Examples","text":"<pre><code>// Get all non-info logs from Collector from the last hour\nCollector\n| where Timestamp &gt; ago(1h)\n| where Body.lvl != \"INF\"\n| project Timestamp, Level=Body.lvl, Msg=Body.msg, Pod=Resource.pod, Host=Resource.host\n</code></pre> <pre><code>// Graph the number of container creations every 15 minutes over the last day, per Host and Cluster\nlet _lookback=ago(1d);\nKubelet\n| where Timestamp &gt; _lookback\n| where Body.message contains \"Syncloop ADD\"\n| make-series Creations=count() default=0 on Timestamp from _lookback to now() step 15m by Host=tostring(Resource.host), Cluster=tostring(Resource.cluster)\n| render timechart \n</code></pre>"},{"location":"quick-start/#setup-dashboards","title":"Setup Dashboards","text":"<p>Any ADX compatible visualization tool can be used to visualize collected telemetry. ADX Dashboards is a simple solution that is native to ADX. You can also use Azure Managed Grafana with the Azure Data Explorer datasource to leverage Grafana's powerful visualization capabilities.</p>"},{"location":"quick-start/#azure-managed-grafana-via-quick-start-script","title":"Azure Managed Grafana via quick-start script","text":"<p>As part of the quick-start script, one can set up an Azure Managed Grafana (AMG) instance. After configuring ADX-Mon on the AKS cluster the script will prompt you about it, and you can provide the name of an existing Grafana instance you have access to or decide to create one. You will also be prompted about importing pre-built dashboards to monitor the AKS cluster.</p> <p>Note: The script tries to create the AMG instance in the same resource group as the ADX cluster. </p> <p>Here's a glimpse of what comes as part of the pre-built dashboards:</p>"},{"location":"quick-start/#api-server","title":"API Server","text":""},{"location":"quick-start/#cluster-info","title":"Cluster Info","text":""},{"location":"quick-start/#metrics-stats","title":"Metrics Stats","text":""},{"location":"quick-start/#namespaces","title":"Namespaces","text":""},{"location":"quick-start/#pods","title":"Pods","text":""},{"location":"designs/log-integration-with-kubernetes/","title":"Log Integration with Kubernetes","text":""},{"location":"designs/log-integration-with-kubernetes/#summary","title":"Summary","text":"<p>Collector is designed to consume container logs based on annotations set on their containing pod (e.g. <code>adx-mon/scrape: true</code>, <code>adx-mon/log-database</code>, <code>adx-mon/log-table</code>). Collector is also intended to enrich logs from containers with the Kubernetes annotations and labels on a pod for use in transforms. This design attempts to hep reconciling the different states that can be observed from the Kubernetes Control Plane and the host while maintaining performance.</p>"},{"location":"designs/log-integration-with-kubernetes/#problem-definitions","title":"Problem definitions","text":""},{"location":"designs/log-integration-with-kubernetes/#dynamic-pod-metadata","title":"Dynamic pod metadata","text":"<p>Metadata associated with pods, and its containers, is dynamic. We need ways to communicate this data to the log enrichment process in a timely manner without impacting the hot path of reading and batching logs. Maintaining a shared data structure and synchronizing with mutexes is too slow for this system.</p>"},{"location":"designs/log-integration-with-kubernetes/#cleaning-tailers-and-on-disk-state-without-dropping-end-of-process-logs","title":"Cleaning tailers and on-disk state without dropping end-of-process logs","text":"<p>The set of containers on a host is also highly dynamic, making it important to dispose of on-disk storage and retire tailers to not consume resources when containers no longer exist. However, due to the distributed nature of Kubernetes, cleaning this state at the time of container deletion in the control plane makes it likely to miss log messages from these containers on the host.</p> <p>The Kubernetes control plane maintains the metadata associated with a pod, but the logs themselves reside solely on the disk where the pod is running. The process of creating, appending to, and deleting these log files is asynchronous from the control-plane mechanisms of Kubernetes. Pods can take some time to fully shut down and CRI impementations may take some time to fully flush logs to disk. The source of truth for logs is on the host, but the source of truth for information about the source of the logs is maintained in the control plane.</p> <pre><code>flowchart LR\n  subgraph Kubernetes Control Plane\n  api[API Server]\n  end\n\n  subgraph Kubernetes Node\n  kubelet[Kubelet] -- CRUD containers --&gt; cri[CRI Driver]\n  cri -- CRUD log files --&gt; log[Log Files]\n  collector[Collector]\n  end\n\n  api -- CRUD pod --&gt; api\n\n  collector -- pod metadata --&gt; api\n  collector -- tail --&gt; log\n  api -- start/stop pod --&gt; kubelet</code></pre> <p>Some concrete situations we are attempting to compensate for:</p> <ol> <li> <p>The gap in time between a pod being deleted in Kubernetes and the logs from the pod fully flushing to disk and being consumed by Collector.</p> </li> <li> <p>Pods are commonly created and removed from hosts. It is important that we clean state related to deleted pods after we are done with that state, in particular tail cursor files and any other on-disk persistance we use.</p> </li> <li> <p>Collector instances may restart while other pods are also shutting down. Best effort we should be able to consume the last logs of a pod that was shut down even while losing in-memory state or the ability to get pod metadata from the Kubernetes control plane.</p> </li> </ol>"},{"location":"designs/log-integration-with-kubernetes/#approaches","title":"Approaches","text":""},{"location":"designs/log-integration-with-kubernetes/#handling-updates-to-pod-metadata","title":"Handling updates to pod metadata","text":"<p>Pod annotations can be dynamically modified which can affect the attributes we expect to apply to logs, including the expected destination ADX database and table. It can also cause us to start scraping new container logs or to exclude containers from being scraped.</p> <p>We will immediately delete our container metadata and notify <code>TailSource</code> to remove a target when a container is made non-scrapable (e.g. the <code>adx-mon/scrape</code> annotation is removed or set to false), but the container still exists.</p> <p>For changes in container metadata that are included in logs that do not change scrapability, we will send via a channel a new updated set of metadata that should be applied to logs from this container. This channel will be consumed asynchronously by the hot-loop within the tailing process to update its own local state. This avoids the need for mutexes in the hot path, but still allows the tailer to receive these changes in near real-time. This extends the actor metaphor for the tailer, where it consumes 3 types of messages within a single goroutine:</p> <ol> <li>A new log line is ready.</li> <li>We are being shut down.</li> <li>We have gotten a new set of fields we should apply to the log.</li> </ol>"},{"location":"designs/log-integration-with-kubernetes/#delayed-cleaning-of-state","title":"Delayed cleaning of state","text":"<p>To account for this skew, we will preserve Pod metadata locally on disk (like log file cursors). This will allow us to access this metadata at any time, even when the Kubernetes API server no longer preserves this information. To clean these files, we will utilize an expiration time placed within this pod metadata file that will be reaped in a polling fashion. This expiration date will be set with a future timestamp (e.g. 10m) by a few flows:</p> <ol> <li>When we recieve a notification from the Kubernetes API server that a container has been deleted.</li> <li>On startup, after inspecting all of the on-disk container metadata we observe that the container itself no longer exists.</li> </ol> <p>When our periodic garbage collection process discovers that a piece of container metadata is past expiration time, it will delete the file and notify <code>TailSource</code> to stop the tailing process for that file and to remove the cursor file. This gap between preserving an expiration timestamp and removing the tailer and its state gives collector time to consume the rest of the logs from a container on a best-effort basis, even in the face of restarts.</p>"},{"location":"designs/schema-etl/","title":"Custom Schema ETL","text":""},{"location":"designs/schema-etl/#summary","title":"Summary","text":"<p>Logs are ingested into Tables using OTLP schema. There are several limitations in querying OTLP directly that can be solved by supporting custom schema definitions.</p>"},{"location":"designs/schema-etl/#solutions","title":"Solutions","text":"<p>Below we present two ways of defining a schema and two ways of implementing those schemas.</p>"},{"location":"designs/schema-etl/#schema-definition","title":"Schema Definition","text":"<ul> <li>CRD that specifies a Table's schema</li> <li>CRD that generically specifies KQL functions</li> </ul>"},{"location":"designs/schema-etl/#etl","title":"ETL","text":"<ul> <li> <p>Update Policies Leveraging the Medallion architecture logs are ingested into preliminary Tables using the OTLP schema definition and update-policies are leveraged to perform ETL operations against the preliminary Table that then emit the logs with custom schemas applied before being stored in their final destinations. We have a great deal of experience with update-policies and have found several limitations, such as failed ingestions due to schema missalignment and increased runtime resource expenses. We have therefore decided to try another route.</p> </li> <li> <p>Views A View with the same name as a Table is defined, where a custom schema is defined and realized at query time. This means the content is stored as OTLP but able to be queried with a user defined schema. There will be a query-time performance penalty with this approach that we need to measure and determine if it's acceptable. </p> </li> </ul>"},{"location":"designs/schema-etl/#proposed-solution","title":"Proposed Solution","text":"<p>We will define a CRD that enables a user to define a KQL Function, with an optional parameter to specify the Function as being a View, whereby ETL operations within the View will present the user with their desired schema at the time of query.</p>"},{"location":"designs/schema-etl/#crd","title":"CRD","text":"<p>Our CRD could simply enable a user to specify any arbitrary KQL; however, to prevent admin commands from being executed, we'll instead specify all the possible fields for a Function and construct the KQL scaffolding ourselves.</p> <p>The CRD definition is as follows:</p> <pre><code>---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: v0.16.1\n  name: functions.adx-mon.azure.com\nspec:\n  group: adx-mon.azure.com\n  names:\n    kind: Function\n    listKind: FunctionList\n    plural: functions\n    singular: function\n  scope: Namespaced\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: Function defines a KQL function to be maintained in the Kusto\n          cluster\n        properties:\n          apiVersion:\n            description: |-\n              APIVersion defines the versioned schema of this representation of an object.\n              Servers should convert recognized schemas to the latest internal value, and\n              may reject unrecognized values.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n            type: string\n          kind:\n            description: |-\n              Kind is a string value representing the REST resource this object represents.\n              Servers may infer this from the endpoint the client submits requests to.\n              Cannot be updated.\n              In CamelCase.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: FunctionSpec defines the desired state of Function\n            properties:\n              body:\n                description: Body is the body of the function\n                type: string\n              database:\n                description: Database is the name of the database in which the function\n                  will be created\n                type: string\n              docString:\n                description: DocString is the documentation string for the function\n                type: string\n              folder:\n                description: Folder is the folder in which the function will be created\n                type: string\n              isView:\n                description: IsView is a flag indicating whether the function is a\n                  view\n                type: boolean\n              name:\n                description: Name is the name of the function\n                type: string\n              parameters:\n                description: Parameters is a list of parameters for the function\n                items:\n                  properties:\n                    name:\n                      type: string\n                    type:\n                      type: string\n                  required:\n                  - name\n                  - type\n                  type: object\n                type: array\n              table:\n                description: |-\n                  Table is the name of the table in which the function will be created. We must\n                  specify a table if the function is a view, otherwise the Table name is optional.\n                type: string\n            required:\n            - body\n            - database\n            - name\n            type: object\n          status:\n            description: FunctionStatus defines the observed state of Function\n            properties:\n              lastTimeReconciled:\n                description: LastTimeReconciled is the last time the Function was\n                  reconciled\n                format: date-time\n                type: string\n              message:\n                description: Message is a human-readable message indicating details\n                  about the Function\n                type: string\n            required:\n            - lastTimeReconciled\n            - message\n            type: object\n        type: object\n    served: true\n    storage: true\n    subresources:\n      status: {}\n</code></pre> <p>A sample use is:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Function\nmetadata:\n  name: samplefn\nspec:\n  name: SampleFn\n  body: |\n    SampleFn\n    | extend Timestamp = todatetime(Body['ts']),\n            Message = tostring(Body['msg']),\n            Labels = Attributes['labels'],\n            Host = tostring(Resource['Host'])\n  database: SomeDatabase\n  table: SampleFn\n  isView: true\n</code></pre> <p>Ingestor would then execute the following </p> <pre><code>.create-or-alter function with ( view=true ) SampleFn () {\n    SampleFn\n    | extend Timestamp = todatetime(Body['ts']),\n            Message = tostring(Body['msg']),\n            Labels = Attributes['labels'],\n            Host = tostring(Resource['Host'])\n}\n</code></pre>"},{"location":"designs/schema-etl/#implementation-details","title":"Implementation Details","text":"<ul> <li>Ingestor already manages the creation of Tables as logs flow through the system, so we'll have Ingestor also manage creation of these Functions because the Table must exist prior to the Function that it references.</li> <li>Alerter already handles CRDs, as much as possible we'll share code between these components so that we're not duplicating a bunch of code.</li> <li>When attempting to reconcile each Function's state, we'll make sure the process is idempotent such that if a Function definition already exists that matches all the parameters found in its CRD, we do not attempt to update its state.</li> </ul>"},{"location":"designs/schema-etl/#questions","title":"Questions","text":"<ul> <li>If Ingestor has a reference to a CRD / Function that is later deleted, do we want to delete the associated Function in Kusto? </li> </ul>"}]}